{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fe8f278",
   "metadata": {},
   "source": [
    "# Comparison of Strategies and ML Methsd for Cryptocurrency and Traditional Equities.\n",
    "#### Manav Agarwal\n",
    "\n",
    "## 00. Data API and Interface\n",
    "\n",
    "This notebook establishes and tests our data collection pipeline for the FinML final project. Since I built my project in a modular fashion, not all functions are here with full code, they are imported from the src folder. I will explain a few algorithms in this and other notebooks as things progress and to make things more readable.\n",
    "\n",
    "### Objectives:\n",
    "1. Test available data sources (Polygon, CMC, yfinance, etc.)\n",
    "2. Collect 2+ years of historical data (Jan 2023 - Aug 2025)\n",
    "3. Validate data quality and completeness\n",
    "4. Establish caching and fallback\n",
    "5. Create data interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2859473e-2c84-49da-b201-0474028eae2a",
   "metadata": {},
   "source": [
    "#### 0. Setup/Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19902952",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Setup and imports"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Custom modules\n",
    "from data.polygon_s3_collector import PolygonS3Collector\n",
    "from data.unified_collector import UnifiedDataCollector\n",
    "from data.batch_collect_data import BatchDataCollector\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a63aa",
   "metadata": {},
   "source": [
    "#### 1. Test Individual Data Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ada448f",
   "metadata": {
    "title": "Test Polygon S3 Access"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Polygon S3 Access\n",
      "Initialized Polygon S3 client\n",
      "Endpoint: https://files.polygon.io\n",
      "Bucket: flatfiles\n",
      "Loading from cache: data\\s3_cache\\crypto_2024-01-01.parquet\n",
      "[OK] Crypto data: 1 records\n",
      "Date range: 0 to 0\n",
      "Columns: ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'symbol']\n",
      "Loading from cache: data\\s3_cache\\stocks_2024-01-02.parquet\n",
      "[OK] Equity data: 2 records\n",
      "  Date range: 0 to 1\n"
     ]
    }
   ],
   "source": [
    "def test_polygon_s3():\n",
    "    print(\"Testing Polygon S3 Access\")\n",
    "    collector = PolygonS3Collector() \n",
    "    # Test crypto data\n",
    "    crypto_test = collector.fetch_crypto_day(\n",
    "        ticker=\"X:BTCUSD\", date=\"2024-01-01\")    \n",
    "    if crypto_test is not None and not crypto_test.empty:\n",
    "        print(f\"[OK] Crypto data: {len(crypto_test)} records\")\n",
    "        print(f\"Date range: {crypto_test.index.min()} to {crypto_test.index.max()}\")\n",
    "        print(f\"Columns: {list(crypto_test.columns)}\")\n",
    "    else:\n",
    "        print(\"[X] Failed to fetch crypto data\")   \n",
    "    # Test equity data\n",
    "    equity_test = collector.fetch_stock_day(\n",
    "        ticker=\"SPY\",\n",
    "        date=\"2024-01-02\")  # Market closed Jan 1)    \n",
    "    if equity_test is not None and not equity_test.empty:\n",
    "        print(f\"[OK] Equity data: {len(equity_test)} records\")\n",
    "        print(f\"  Date range: {equity_test.index.min()} to {equity_test.index.max()}\")\n",
    "    else:\n",
    "        print(\"[X] Failed to fetch equity data\")    \n",
    "    return collector\n",
    "polygon_collector = test_polygon_s3()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30f63ab",
   "metadata": {},
   "source": [
    "#### 2. Define Target Symbols and Date Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fbaa873",
   "metadata": {
    "title": "Configuration"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collection Configuration:\n",
      "  Crypto: ['BTCUSD', 'ETHUSD', 'SOLUSD', 'ADAUSD', 'XRPUSD']\n",
      "  Equity: ['SPY', 'QQQ', 'IWM', 'DIA', 'VTI']\n",
      "  Period: 2023-01-01 to 2025-08-01\n",
      "  Reserved regime: 2025-01-01 onwards\n"
     ]
    }
   ],
   "source": [
    "config = {\n",
    "    \"crypto_symbols\": [\"BTCUSD\", \"ETHUSD\", \"SOLUSD\", \"ADAUSD\", \"XRPUSD\"],\n",
    "    \"equity_symbols\": [\"SPY\", \"QQQ\", \"IWM\", \"DIA\", \"VTI\"],\n",
    "    \"start_date\": \"2023-01-01\",\n",
    "    \"end_date\": \"2025-08-01\",\n",
    "    \"regime_change_start\": \"2025-01-01\",  # Reserved for regime analysis\n",
    "    \"cache_dir\": \"../data/ml_comparison_cache\"\n",
    "}\n",
    "\n",
    "print(\"Data Collection Configuration:\")\n",
    "print(f\"  Crypto: {config['crypto_symbols']}\")\n",
    "print(f\"  Equity: {config['equity_symbols']}\")\n",
    "print(f\"  Period: {config['start_date']} to {config['end_date']}\")\n",
    "print(f\"  Reserved regime: {config['regime_change_start']} onwards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc4da0",
   "metadata": {},
   "source": [
    "#### 3. Batch Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0d580b2",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Initialize batch collector"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:data.batch_collect_data:Using device: cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Polygon S3 client\n",
      "Endpoint: https://files.polygon.io\n",
      "Bucket: flatfiles\n",
      "Batch collector initialized\n",
      "Cache directory: ..\\data\\ml_comparison_cache\n"
     ]
    }
   ],
   "source": [
    "# Initialize batch collector\n",
    "batch_collector = BatchDataCollector(cache_dir=config['cache_dir'])\n",
    "\n",
    "print(f\"Batch collector initialized\")\n",
    "print(f\"Cache directory: {batch_collector.cache_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a4415-4fd2-4b20-b095-5d1c89551d09",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def collect_sample_data():\n",
    "    print(\"Collecting Sample Data (1 month)\")\n",
    "    sample_start = \"2024-01-01\"\n",
    "    sample_end = \"2024-01-31\"\n",
    "    results = {}\n",
    "    # Collect crypto\n",
    "    for symbol in config['crypto_symbols'][:2]: e\n",
    "        print(f\"\\nCollecting {symbol}...\")\n",
    "        ticker = f\"X:{symbol}\"\n",
    "        data = batch_collector.crypto_collector.fetch_aggregated_data(symbol=symbol, \n",
    "            start=sample_start, end=sample_end, market='crypto', timeframe='hour')\n",
    "        if data is not None and not data.empty:\n",
    "            results[symbol] = data\n",
    "            print(f\"  [OK] {len(data)} records collected\")\n",
    "            print(f\"  Date range: {data.index.min()} to {data.index.max()}\")\n",
    "        else:\n",
    "            print(f\"  [X] Failed to collect {symbol}\")\n",
    "    # Just SPY and QQQ for sample equity/indice            \n",
    "    for symbol in config['equity_symbols'][:2]: \n",
    "        print(f\"\\nCollecting {symbol}...\")\n",
    "        data = batch_collector.crypto_collector.fetch_aggregated_data(\n",
    "            symbol=symbol, start=sample_start, end=sample_end, market='stocks', timeframe='day')\n",
    "        if data is not None and not data.empty:\n",
    "            results[symbol] = data\n",
    "            print(f\"  [OK] {len(data)} records collected\")\n",
    "        else:\n",
    "            print(f\"  [X] Failed to collect {symbol}\")\n",
    "    return results\n",
    "    \n",
    "sample_data = collect_sample_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f36ac-a2df-4ace-bd2c-ffb6492006ef",
   "metadata": {},
   "source": [
    "We get the continuous crypto data and \"No data available for 2024-01-01 in stocks\" due to holiday, looks like its working like expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1519c37a",
   "metadata": {},
   "source": [
    "#### 4. Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d3820e9",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Analyze data quality"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Quality Analysis\n",
      "==================================================================================================================================\n",
      "   symbol  records  start_date  end_date  missing_values  missing_pct  zero_volume_pct            price_range  avg_daily_volume\n",
      "0  BTCUSD      744           0       743               0      0.00000         0.000000  $38787.00 - $48633.40      9.894472e+02\n",
      "1  ETHUSD      744           0       743               0      0.00000         0.000000    $2183.61 - $2706.55      6.016779e+03\n",
      "2     SPY       31           0        30              20      9.21659        16.129032      $467.75 - $491.05      5.311149e+07\n",
      "3     QQQ       31           0        30              20      9.21659        16.129032      $396.54 - $428.49      3.086514e+07\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_quality(data_dict):\n",
    "    print(\"Data Quality Analysis\")\n",
    "    print(\"=\"*130)\n",
    "    quality_report = pd.DataFrame()\n",
    "    for symbol, data in data_dict.items():\n",
    "        report = {\n",
    "            'symbol': symbol, 'records': len(data), 'start_date': data.index.min(),\n",
    "            'end_date': data.index.max(), 'missing_values': data.isnull().sum().sum(),\n",
    "            'missing_pct': (data.isnull().sum().sum() / (len(data) * len(data.columns))) * 100,\n",
    "            'zero_volume_pct': (data['volume'] == 0).sum() / len(data) * 100 if 'volume' in data.columns else 0,\n",
    "            'price_range': f\"${data['close'].min():.2f} - ${data['close'].max():.2f}\",\n",
    "            'avg_daily_volume': data['volume'].mean() if 'volume' in data.columns else 0}\n",
    "        quality_report = pd.concat([quality_report, pd.DataFrame([report])], ignore_index=True)\n",
    "    print(quality_report.to_string())\n",
    "    return quality_report\n",
    "if sample_data:\n",
    "    quality_report = analyze_data_quality(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fc3f0",
   "metadata": {},
   "source": [
    "#### 5. Feature Engineering Test/Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e1b9e29",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Preview features for sample data"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Engineering Preview\n",
      "==============================================================================================================\n",
      "Original columns: 7\n",
      "After feature engineering: 107\n",
      "\n",
      "Price Features (7):\n",
      "  ['returns', 'log_returns', 'price_position', 'price_range_position_7', 'price_range_position_14']...\n",
      "\n",
      "Volume Features (9):\n",
      "  ['volume', 'volume_sma_7', 'volume_ratio_7', 'volume_sma_14', 'volume_ratio_14']...\n",
      "\n",
      "Technical Features (29):\n",
      "  ['sma_7', 'ema_7', 'close_to_sma_7', 'close_to_ema_7', 'sma_14']...\n",
      "\n",
      "Volatility Features (10):\n",
      "  ['atr', 'atr_ratio', 'volatility_7', 'volatility_ann_7', 'volatility_14']...\n",
      "\n",
      "Market Structure Features (0):\n",
      "  []\n"
     ]
    }
   ],
   "source": [
    "def preview_features():\n",
    "    from features.feature_engineering import FeatureEngineer\n",
    "    print(\"Feature Engineering Preview\")\n",
    "    print(\"=\"*110)\n",
    "    \n",
    "    fe = FeatureEngineer()\n",
    "    if 'BTCUSD' in sample_data:\n",
    "        btc_data = sample_data['BTCUSD'].copy() \n",
    "        # Engineer features\n",
    "        btc_features = fe.create_features(btc_data, 'BTCUSD')\n",
    "        print(f\"Original columns: {len(btc_data.columns)}\")\n",
    "        print(f\"After feature engineering: {len(btc_features.columns)}\")\n",
    "        feature_categories = {\n",
    "            'Price': [col for col in btc_features.columns if 'return' in col or 'price' in col],\n",
    "            'Volume': [col for col in btc_features.columns if 'volume' in col],\n",
    "            'Technical': [col for col in btc_features.columns if any(ind in col for ind in ['rsi', 'macd', 'bb', 'sma', 'ema'])],\n",
    "            'Volatility': [col for col in btc_features.columns if 'volatility' in col or 'atr' in col],\n",
    "            'Market Structure': [col for col in btc_features.columns if any(ms in col for ms in ['spread', 'depth', 'imbalance'])]}\n",
    "        for category, features in feature_categories.items():\n",
    "            print(f\"\\n{category} Features ({len(features)}):\")\n",
    "            print(f\"  {features[:5]}...\" if len(features) > 5 else f\"  {features}\")\n",
    "        return btc_features\n",
    "    return None\n",
    "\n",
    "btc_features = preview_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798005b",
   "metadata": {},
   "source": [
    "#### 6. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e0a86f8",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Analyze correlations"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Asset Correlation Analysis\n",
      "========================================\n",
      "\n",
      "Correlation Matrix:\n",
      "        BTCUSD  ETHUSD    SPY    QQQ\n",
      "BTCUSD   1.000   0.833  0.043 -0.048\n",
      "ETHUSD   0.833   1.000  0.032 -0.076\n",
      "SPY      0.043   0.032  1.000  0.916\n",
      "QQQ     -0.048  -0.076  0.916  1.000\n",
      "\n",
      "Top Correlations:\n",
      "  SPY-QQQ: 0.916\n",
      "  BTCUSD-ETHUSD: 0.833\n",
      "  ETHUSD-QQQ: -0.076\n",
      "  BTCUSD-QQQ: -0.048\n",
      "  BTCUSD-SPY: 0.043\n"
     ]
    }
   ],
   "source": [
    "def analyze_correlations():\n",
    "    print(\"Asset Correlation Analysis\")\n",
    "    print(\"=\"*40)\n",
    "    if len(sample_data) < 2:\n",
    "        print(\"Need at least 2 assets for correlation analysis\")\n",
    "        return None \n",
    "    # Prepare returns data\n",
    "    returns_data = pd.DataFrame()\n",
    "    for symbol, data in sample_data.items():\n",
    "        if 'close' in data.columns:\n",
    "            returns = data['close'].pct_change().dropna()\n",
    "            returns_data[symbol] = returns\n",
    "    # Calculate correlation matrix\n",
    "    if not returns_data.empty:\n",
    "        # Align indices\n",
    "        returns_data = returns_data.dropna()\n",
    "        if len(returns_data) > 0:\n",
    "            corr_matrix = returns_data.corr()   \n",
    "            print(\"\\nCorrelation Matrix:\")\n",
    "            print(corr_matrix.round(3).to_string())\n",
    "            # Find highest correlations\n",
    "            corr_pairs = []\n",
    "            for i in range(len(corr_matrix.columns)):\n",
    "                for j in range(i+1, len(corr_matrix.columns)):\n",
    "                    corr_pairs.append({\n",
    "                        'pair': f\"{corr_matrix.columns[i]}-{corr_matrix.columns[j]}\",\n",
    "                        'correlation': corr_matrix.iloc[i, j]})\n",
    "            corr_pairs = sorted(corr_pairs, key=lambda x: abs(x['correlation']), reverse=True)\n",
    "            print(\"\\nTop Correlations:\")\n",
    "            for pair in corr_pairs[:5]:\n",
    "                print(f\"  {pair['pair']}: {pair['correlation']:.3f}\")     \n",
    "            return corr_matrix\n",
    "    return None\n",
    "\n",
    "corr_matrix = analyze_correlations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b9712",
   "metadata": {},
   "source": [
    "#### 7. Full Data Collection Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m254xlvyxq",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_crypto_from_yfinance(symbol, start_date, end_date):\n",
    "    import yfinance as yf\n",
    "    \n",
    "    yf_mapping = {\n",
    "        'BTCUSD': 'BTC-USD',\n",
    "        'ETHUSD': 'ETH-USD',\n",
    "        'SOLUSD': 'SOL-USD',\n",
    "        'XRPUSD': 'XRP-USD',\n",
    "        'ADAUSD': 'ADA-USD'\n",
    "    }\n",
    "    \n",
    "    if symbol not in yf_mapping:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        ticker = yf.Ticker(yf_mapping[symbol])\n",
    "        data = ticker.history(start=start_date, end=end_date, interval='1h')\n",
    "        \n",
    "        if not data.empty:\n",
    "            # Standardize column names\n",
    "            data.columns = [col.lower() for col in data.columns]\n",
    "            data['symbol'] = symbol\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        print(f\"yfinance error for {symbol}: {str(e)[:50]}\")\n",
    "        return None\n",
    "\n",
    "def merge_data_sources(base_data, new_data):\n",
    "    if not isinstance(base_data.index, pd.DatetimeIndex):\n",
    "        if 'timestamp' in base_data.columns:\n",
    "            base_data.index = pd.to_datetime(base_data['timestamp'])\n",
    "    \n",
    "    if not isinstance(new_data.index, pd.DatetimeIndex):\n",
    "        if 'timestamp' in new_data.columns:\n",
    "            new_data.index = pd.to_datetime(new_data['timestamp'])\n",
    "    \n",
    "    combined = pd.concat([base_data, new_data])\n",
    "    combined = combined[~combined.index.duplicated(keep='first')]\n",
    "    combined = combined.sort_index()\n",
    "    \n",
    "    return combined\n",
    "\n",
    "def collect_with_multi_source_fallback(symbol, start_date, end_date, cache_dir=\"../data/ml_comparison_cache\"):\n",
    "    from data.collect_ml_comparison_data import MLComparisonDataCollector\n",
    "    from data.unified_collector import UnifiedDataCollector\n",
    "    from data.polygon_s3_collector import PolygonS3Collector\n",
    "    from datetime import timedelta\n",
    "    print(f\"Collecting {symbol} with fallback\")\n",
    "    all_data_sources = {}\n",
    "    try:\n",
    "        print(\"1. Trying MLComparisonDataCollector...\")\n",
    "        ml_collector = MLComparisonDataCollector(cache_dir=cache_dir)\n",
    "        ml_data = ml_collector.collect_comparison_data()\n",
    "        if symbol in ml_data and ml_data[symbol] is not None and not ml_data[symbol].empty:\n",
    "            all_data_sources['ml_collector'] = ml_data[symbol]\n",
    "            print(f\"Got {len(ml_data[symbol])} records\")\n",
    "        else:\n",
    "            print(f\"No data from MLComparisonDataCollector\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:50]}\")\n",
    "\n",
    "    # 2. Try Polygon S3 directly\n",
    "    try:\n",
    "        print(\"2. Trying Polygon S3...\")\n",
    "        if symbol in ['BTCUSD', 'ETHUSD', 'SOLUSD', 'ADAUSD', 'XRPUSD']:\n",
    "            polygon_collector = PolygonS3Collector(cache_dir=cache_dir)\n",
    "            # Use the proper ticker format\n",
    "            ticker_mapping = {\n",
    "                'BTCUSD': 'X:BTC-USD',\n",
    "                'ETHUSD': 'X:ETH-USD',\n",
    "                'SOLUSD': 'X:SOL-USD',\n",
    "                'XRPUSD': 'X:XRP-USD',\n",
    "                'ADAUSD': 'X:ADA-USD'\n",
    "            }\n",
    "            ticker = ticker_mapping.get(symbol, f\"X:{symbol}\")\n",
    "            try:\n",
    "                bulk_data = polygon_collector.fetch_bulk_historical(\n",
    "                    symbols=[symbol],\n",
    "                    start=start_date,\n",
    "                    end=end_date,\n",
    "                    market='crypto'\n",
    "                )\n",
    "                if symbol in bulk_data and not bulk_data[symbol].empty:\n",
    "                    all_data_sources['polygon'] = bulk_data[symbol]\n",
    "                    print(f\"Got {len(bulk_data[symbol])} records\")\n",
    "                else:\n",
    "                    print(f\"No data from Polygon S3\")\n",
    "            except Exception as e:\n",
    "                print(f\"Polygon S3 error: {str(e)[:50]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:50]}\")\n",
    "\n",
    "    try:\n",
    "        print(\"3. Trying UnifiedDataCollector...\")\n",
    "        unified = UnifiedDataCollector(cache_dir=cache_dir)\n",
    "\n",
    "        if symbol in ['BTCUSD', 'ETHUSD', 'SOLUSD', 'ADAUSD', 'XRPUSD']:\n",
    "            timespan = 'hour'\n",
    "        else:\n",
    "            timespan = 'day'\n",
    "\n",
    "        unified_data = unified.fetch_data(\n",
    "            symbol=symbol,\n",
    "            start=start_date,\n",
    "            end=end_date,\n",
    "            timespan=timespan\n",
    "        )\n",
    "\n",
    "        if unified_data is not None and not unified_data.empty:\n",
    "            all_data_sources['unified'] = unified_data\n",
    "            print(f\"Got {len(unified_data)} records\")\n",
    "        else:\n",
    "            print(f\"No data from UnifiedDataCollector\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:50]}\")\n",
    "\n",
    "    try:\n",
    "        print(\"4. Trying yfinance...\")\n",
    "        yf_data = collect_crypto_from_yfinance(symbol, start_date, end_date)\n",
    "        if yf_data is not None and not yf_data.empty:\n",
    "            all_data_sources['yfinance'] = yf_data\n",
    "            print(f\"Got {len(yf_data)} records\")\n",
    "        else:\n",
    "            print(f\"No data from yfinance\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)[:50]}\")\n",
    "\n",
    "    if not all_data_sources:\n",
    "        print(f\"\\nFailed to collect any data for {symbol}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"\\nMerging {len(all_data_sources)} data sources...\")\n",
    "\n",
    "    sorted_sources = sorted(all_data_sources.items(), key=lambda x: len(x[1]), reverse=True)\n",
    "    merged_data = sorted_sources[0][1].copy()\n",
    "    print(f\"   Base: {sorted_sources[0][0]} with {len(merged_data)} records\")\n",
    "    for source_name, source_data in sorted_sources[1:]:\n",
    "        print(f\"   Merging {source_name} ({len(source_data)} records)...\")\n",
    "        merged_data = merge_data_sources(merged_data, source_data)\n",
    "    merged_data.columns = [col.lower() for col in merged_data.columns]\n",
    "    required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "    for col in required_cols:\n",
    "        if col not in merged_data.columns:\n",
    "            print(f\"   Warning: Missing column {col}, adding with zeros\")\n",
    "            merged_data[col] = 0\n",
    "    if 'symbol' not in merged_data.columns:\n",
    "        merged_data['symbol'] = symbol\n",
    "\n",
    "    print(f\"\\nâœ“ Final merged data: {len(merged_data)} records\")\n",
    "    if hasattr(merged_data.index, 'min'):\n",
    "        print(f\"  Date range: {merged_data.index.min()} to {merged_data.index.max()}\")\n",
    "    return merged_data\n",
    "\n",
    "def run_full_collection_enhanced(test_mode=False):\n",
    "    print(\"FULL DATA COLLECTION\")\n",
    "    if test_mode:\n",
    "        start = \"2024-01-01\"\n",
    "        end = \"2024-01-07\"\n",
    "        print(f\"Test mode: Collecting {start} to {end}\")\n",
    "    else:\n",
    "        start = config['start_date']\n",
    "        end = config['end_date']\n",
    "        print(f\"Full mode: Collecting {start} to {end}\")\n",
    "        print(\"...\")\n",
    "    from data.collect_ml_comparison_data import MLComparisonDataCollector\n",
    "    import yfinance as yf\n",
    "    ml_collector = MLComparisonDataCollector(cache_dir=config['cache_dir'])\n",
    "    unified = UnifiedDataCollector(cache_dir=config['cache_dir'])\n",
    "    all_data = {}\n",
    "    problem_symbols = ['SOLUSD', 'XRPUSD'] \n",
    "    print(\"\\n1. Primary collection with MLComparisonDataCollector...\")\n",
    "    ml_data = ml_collector.collect_comparison_data()\n",
    "    \n",
    "    if ml_data:\n",
    "        for symbol in config['crypto_symbols'] + config['equity_symbols']:\n",
    "            if symbol in ml_data and ml_data[symbol] is not None and not ml_data[symbol].empty:\n",
    "                all_data[symbol] = ml_data[symbol]\n",
    "                print(f\"{symbol}: {len(ml_data[symbol])} records\")\n",
    "                \n",
    "                if symbol in problem_symbols and len(ml_data[symbol]) < 10000:\n",
    "                    print(f\"{symbol} has incomplete data\")\n",
    "            else:\n",
    "                print(f\"{symbol}: No data from primary\")\n",
    "    \n",
    "    # Special handling for problematic symbols\n",
    "    print(\"\\n2. Multi source collection for problems...\")\n",
    "    for symbol in problem_symbols:\n",
    "        if symbol not in all_data or len(all_data.get(symbol, pd.DataFrame())) < 10000:\n",
    "            print(f\"\\nCollecting {symbol} from multiple sources:\")\n",
    "            complete_data = collect_with_multi_source_fallback(\n",
    "                symbol=symbol,\n",
    "                start_date=start,\n",
    "                end_date=end,\n",
    "                cache_dir=config['cache_dir']\n",
    "            )\n",
    "            if complete_data is not None:\n",
    "                # Replace or add the complete data\n",
    "                all_data[symbol] = complete_data\n",
    "                print(f\"{symbol}: Successfully collected {len(complete_data)} records\")\n",
    "            else:\n",
    "                print(f\"{symbol}: Failed to collect sufficient data\")\n",
    "    \n",
    "    failed_symbols = [s for s in config['crypto_symbols'] + config['equity_symbols'] \n",
    "                     if s not in all_data or all_data[s] is None or all_data[s].empty]\n",
    "    if failed_symbols:\n",
    "        print(f\"\\n3. Standard fallback: {failed_symbols}\")\n",
    "        \n",
    "        for symbol in failed_symbols:\n",
    "            print(f\"  Collecting {symbol}...\") \n",
    "            if symbol in config['crypto_symbols']:\n",
    "                yf_data = collect_crypto_from_yfinance(symbol, start, end)\n",
    "                if yf_data is not None and not yf_data.empty:\n",
    "                    all_data[symbol] = yf_data\n",
    "                    print(f\"{symbol}: {len(yf_data)} records via yfinance\")\n",
    "                    continue\n",
    "            timespan = 'hour' if symbol in config['crypto_symbols'] else 'day'\n",
    "            alt_symbol = f\"X:{symbol}\" if symbol in config['crypto_symbols'] else symbol\n",
    "            data = unified.fetch_data(\n",
    "                symbol=alt_symbol,\n",
    "                start=start,\n",
    "                end=end,\n",
    "                timespan=timespan,\n",
    "                use_cache=True\n",
    "            )\n",
    "            if data is not None and not data.empty:\n",
    "                all_data[symbol] = data\n",
    "                print(f\"{symbol}: {len(data)} records via UnifiedCollector\")\n",
    "            else:\n",
    "                print(f\"{symbol}: Failed all collection attempts\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"COLLECTION SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    if all_data:\n",
    "        success_count = len(all_data)\n",
    "        total_count = len(config['crypto_symbols']) + len(config['equity_symbols'])\n",
    "        print(f\"Successfully collected: {success_count}/{total_count} symbols\")\n",
    "        summary_data = []\n",
    "        for symbol, data in all_data.items():\n",
    "            if isinstance(data.index, pd.DatetimeIndex):\n",
    "                start_date = data.index.min()\n",
    "                end_date = data.index.max()\n",
    "            else:\n",
    "                start_date = pd.to_datetime(data.index.min())\n",
    "                end_date = pd.to_datetime(data.index.max())\n",
    "            \n",
    "            # Calculate completeness\n",
    "            days = (end_date - start_date).days\n",
    "            if symbol in config['crypto_symbols']:\n",
    "                expected_records = days * 24  # Hourly\n",
    "            else:\n",
    "                expected_records = days  # Daily\n",
    "            completeness = (len(data) / expected_records * 100) if expected_records > 0 else 0\n",
    "            summary_data.append({\n",
    "                'Symbol': symbol,\n",
    "                'Records': len(data),\n",
    "                'Start': start_date.strftime('%Y-%m-%d'),\n",
    "                'End': end_date.strftime('%Y-%m-%d'),\n",
    "                'Days': days,\n",
    "                'Completeness': f\"{completeness:.1f}%\"\n",
    "            })\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        print(\"\\n\" + summary_df.to_string(index=False))\n",
    "        issues = summary_df[summary_df['Records'] < 5000]\n",
    "        if not issues.empty:\n",
    "            print(issues[['Symbol', 'Records', 'Completeness']].to_string(index=False))\n",
    "        else:\n",
    "            print(\"\\nAll symbols have sufficient data!\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata = {\n",
    "            'collection_date': datetime.now().isoformat(),\n",
    "            'symbols': list(all_data.keys()),\n",
    "            'start_date': start,\n",
    "            'end_date': end,\n",
    "            'test_mode': test_mode,\n",
    "            'summary': summary_data\n",
    "        }\n",
    "        \n",
    "        metadata_path = Path(config['cache_dir']) / 'collection_metadata.json'\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        print(f\"\\nMetadata saved to {metadata_path}\")\n",
    "        \n",
    "        # Update test_data global variable\n",
    "        global test_data\n",
    "        test_data = all_data\n",
    "        \n",
    "        return all_data\n",
    "    else:\n",
    "        print(\"Collection failed for all symbols\")\n",
    "        return None\n",
    "\n",
    "print(\"Starting enhanced data collection with multi-source fallback...\")\n",
    "test_data = run_full_collection_enhanced(test_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7yy59xqf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnose_data_issues():\n",
    "    print(\"DATA AVAILABILITY DIAGNOSTIC\")\n",
    "    print(\"=\"*80)\n",
    "    problem_symbols = ['SOLUSD', 'XRPUSD']\n",
    "    for symbol in problem_symbols:\n",
    "        print(f\"\\nDiagnosing {symbol}:\")        \n",
    "        test_ranges = [\n",
    "            (\"2023-01-01\", \"2023-03-01\"),\n",
    "            (\"2023-03-01\", \"2023-06-01\"),\n",
    "            (\"2023-06-01\", \"2023-09-01\"),\n",
    "            (\"2023-09-01\", \"2024-01-01\"),\n",
    "            (\"2024-01-01\", \"2024-06-01\"),\n",
    "            (\"2024-06-01\", \"2025-01-01\"),\n",
    "            (\"2025-01-01\", \"2025-08-01\")\n",
    "        ]\n",
    "        for start, end in test_ranges:\n",
    "            try:\n",
    "                ticker = f\"X:{symbol}\"\n",
    "                data = polygon_collector.fetch_aggregated_data(\n",
    "                    symbol=ticker,\n",
    "                    start=start,\n",
    "                    end=end,\n",
    "                    market='crypto',\n",
    "                    timeframe='hour'\n",
    "                )\n",
    "                if data is not None and not data.empty:\n",
    "                    print(f\"{start} to {end}: {len(data)} records found\")\n",
    "                else:\n",
    "                    print(f\"{start} to {end}: No data\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"{start} to {end}: Error - {str(e)[:50]}\")\n",
    "        \n",
    "        # Try alternative tickers\n",
    "        print(f\"\\n  Testing alternative tickers for {symbol}:\")\n",
    "        alt_tickers = [\n",
    "            f\"X:{symbol}\",\n",
    "            symbol.replace(\"USD\", \"-USD\"),\n",
    "            symbol.replace(\"USD\", \"\"),\n",
    "            symbol[:3] + \"-USD\",\n",
    "            symbol[:3]\n",
    "        ]\n",
    "        for alt in alt_tickers:\n",
    "            try:\n",
    "                data = unified.fetch_data(\n",
    "                    symbol=alt,\n",
    "                    start=\"2024-01-01\",\n",
    "                    end=\"2024-01-07\",\n",
    "                    timespan='hour',\n",
    "                    use_cache=False\n",
    "                )\n",
    "                if data is not None and not data.empty:\n",
    "                    print(f\"'{alt}': {len(data)} records\")\n",
    "                else:\n",
    "                    print(f\"'{alt}': No data\")\n",
    "            except:\n",
    "                print(f\"'{alt}': Failed\")\n",
    "\n",
    "# Run diagnostics\n",
    "diagnose_data_issues()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qhfieq001h",
   "metadata": {},
   "source": [
    "#### Backup Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fq1zp9fbxl",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing yfinance fallback for problematic symbols:\n",
      "------------------------------------------------------------\n",
      "Fetching SOLUSD from yfinance as SOL-USD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:yfinance:$SOL-USD: possibly delisted; no price data found  (1h 2023-01-01 -> 2025-08-01) (Yahoo error = \"1h data not available for startTime=1672531200 and endTime=1754006400. The requested range must be within the last 730 days.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No data returned from yfinance\n",
      "SOLUSD: Failed to retrieve from yfinance\n",
      "Fetching XRPUSD from yfinance as XRP-USD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:yfinance:$XRP-USD: possibly delisted; no price data found  (1h 2023-01-01 -> 2025-08-01) (Yahoo error = \"1h data not available for startTime=1672531200 and endTime=1754006400. The requested range must be within the last 730 days.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  No data returned from yfinance\n",
      "XRPUSD: Failed to retrieve from yfinance\n"
     ]
    }
   ],
   "source": [
    "def collect_crypto_from_yfinance(symbol, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Collect crypto data from yfinance as fallback\n",
    "    Formats to match our standard structure\n",
    "    \"\"\"\n",
    "    import yfinance as yf\n",
    "    \n",
    "    # Map our symbols to yfinance tickers\n",
    "    yf_mapping = {\n",
    "        'SOLUSD': 'SOL-USD',\n",
    "        'XRPUSD': 'XRP-USD',\n",
    "        'BTCUSD': 'BTC-USD',\n",
    "        'ETHUSD': 'ETH-USD',\n",
    "        'ADAUSD': 'ADA-USD'\n",
    "    }\n",
    "    \n",
    "    if symbol not in yf_mapping:\n",
    "        print(f\"No yfinance mapping for {symbol}\")\n",
    "        return None\n",
    "    \n",
    "    yf_ticker = yf_mapping[symbol]\n",
    "    print(f\"Fetching {symbol} from yfinance as {yf_ticker}...\")\n",
    "    \n",
    "    try:\n",
    "        # Download data with hourly intervals if available\n",
    "        ticker = yf.Ticker(yf_ticker)\n",
    "        \n",
    "        # Try hourly data first (1h interval)\n",
    "        try:\n",
    "            data = ticker.history(\n",
    "                start=start_date,\n",
    "                end=end_date,\n",
    "                interval=\"1h\",\n",
    "                auto_adjust=True,\n",
    "                prepost=True\n",
    "            )\n",
    "        except:\n",
    "            # Fall back to daily data if hourly not available\n",
    "            print(f\"  Hourly data not available, using daily...\")\n",
    "            data = ticker.history(\n",
    "                start=start_date,\n",
    "                end=end_date,\n",
    "                interval=\"1d\",\n",
    "                auto_adjust=True\n",
    "            )\n",
    "            \n",
    "            # Resample to hourly (forward fill)\n",
    "            if not data.empty:\n",
    "                # Create hourly index\n",
    "                hourly_index = pd.date_range(\n",
    "                    start=data.index[0],\n",
    "                    end=data.index[-1] + pd.Timedelta(hours=23),\n",
    "                    freq='h'\n",
    "                )\n",
    "                # Reindex and forward fill\n",
    "                data = data.reindex(hourly_index, method='ffill')\n",
    "        \n",
    "        if data.empty:\n",
    "            print(f\"  No data returned from yfinance\")\n",
    "            return None\n",
    "        \n",
    "        # Format to match our standard structure\n",
    "        formatted_data = pd.DataFrame({\n",
    "            'open': data['Open'],\n",
    "            'high': data['High'],\n",
    "            'low': data['Low'],\n",
    "            'close': data['Close'],\n",
    "            'volume': data['Volume'],\n",
    "            'symbol': symbol\n",
    "        })\n",
    "        \n",
    "        # Add timestamp column if index is not already datetime\n",
    "        if not isinstance(formatted_data.index, pd.DatetimeIndex):\n",
    "            formatted_data.index = pd.to_datetime(formatted_data.index)\n",
    "        \n",
    "        formatted_data['timestamp'] = formatted_data.index\n",
    "        \n",
    "        print(f\"Retrieved {len(formatted_data)} records from yfinance\")\n",
    "        return formatted_data\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"yfinance error: {str(e)[:100]}\")\n",
    "        return None\n",
    "\n",
    "# Test yfinance fallback for SOL and XRP\n",
    "print(\"Testing yfinance fallback for problematic symbols:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "test_symbols = ['SOLUSD', 'XRPUSD']\n",
    "yf_data = {}\n",
    "\n",
    "for symbol in test_symbols:\n",
    "    data = collect_crypto_from_yfinance(\n",
    "        symbol=symbol,\n",
    "        start_date=config['start_date'],\n",
    "        end_date=config['end_date']\n",
    "    )\n",
    "    if data is not None:\n",
    "        yf_data[symbol] = data\n",
    "        print(f\"{symbol}: Start={data.index.min()}, End={data.index.max()}, Records={len(data)}\")\n",
    "    else:\n",
    "        print(f\"{symbol}: Failed to retrieve from yfinance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "n1q0ghhjt0p",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing data merging for SOL and XRP:\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def merge_data_sources(primary_data, fallback_data):\n",
    "    if primary_data is None or primary_data.empty:\n",
    "        return fallback_data\n",
    "    if fallback_data is None or fallback_data.empty:\n",
    "        return primary_data\n",
    "    if not isinstance(primary_data.index, pd.DatetimeIndex):\n",
    "        primary_data.index = pd.to_datetime(primary_data.index)\n",
    "    if not isinstance(fallback_data.index, pd.DatetimeIndex):\n",
    "        fallback_data.index = pd.to_datetime(fallback_data.index)\n",
    "    primary_start = primary_data.index.min()\n",
    "    primary_end = primary_data.index.max()\n",
    "    \n",
    "    print(f\"  Primary data: {primary_start} to {primary_end} ({len(primary_data)} records)\")\n",
    "    print(f\"  Fallback data: {fallback_data.index.min()} to {fallback_data.index.max()} ({len(fallback_data)} records)\")\n",
    "    fallback_before = fallback_data[fallback_data.index < primary_start]\n",
    "    fallback_after = fallback_data[fallback_data.index > primary_end]\n",
    "    combined = pd.concat([fallback_before, primary_data, fallback_after], axis=0)\n",
    "    # Remove duplicates, keeping first (primary data)\n",
    "    combined = combined[~combined.index.duplicated(keep='first')]\n",
    "    # Sort by index\n",
    "    combined = combined.sort_index()\n",
    "    print(f\"  Merged data: {combined.index.min()} to {combined.index.max()} ({len(combined)} records)\")\n",
    "    \n",
    "    return combined\n",
    "\n",
    "print(\"\\nTesting data merging for SOL and XRP:\")\n",
    "\n",
    "primary_sol = test_data.get('SOLUSD') if 'test_data' in locals() else None\n",
    "primary_xrp = test_data.get('XRPUSD') if 'test_data' in locals() else None\n",
    "\n",
    "if 'SOLUSD' in yf_data:\n",
    "    print(\"\\nMerging SOL data:\")\n",
    "    merged_sol = merge_data_sources(primary_sol, yf_data['SOLUSD'])\n",
    "    if merged_sol is not None:\n",
    "        print(f\"  Final SOL: {len(merged_sol)} total records\")\n",
    "\n",
    "if 'XRPUSD' in yf_data:\n",
    "    print(\"\\nMerging XRP data:\")\n",
    "    merged_xrp = merge_data_sources(primary_xrp, yf_data['XRPUSD'])\n",
    "    if merged_xrp is not None:\n",
    "        print(f\"  Final XRP: {len(merged_xrp)} total records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amhw0w2gc4n",
   "metadata": {},
   "source": [
    "#### 8. Final Data Export and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x9ug1anrlq",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data validation and export\n",
    "def validate_and_export_data(data_dict, export_dir=\"../data/processed\"):\n",
    "    print(\"FINAL DATA VALIDATION AND EXPORT\")\n",
    "    print(\"=\"*80)\n",
    "    export_path = Path(export_dir)\n",
    "    export_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_results = []\n",
    "    \n",
    "    for symbol, data in data_dict.items():\n",
    "        if data is None or data.empty:\n",
    "            print(f\" {symbol}: No data to export\")\n",
    "            continue\n",
    "        issues = []\n",
    "        # Check for required columns\n",
    "        required_cols = ['open', 'high', 'low', 'close', 'volume']\n",
    "        missing_cols = [col for col in required_cols if col not in data.columns]\n",
    "        if missing_cols:\n",
    "            issues.append(f\"Missing columns: {missing_cols}\")\n",
    "        # Check for data gaps\n",
    "        if isinstance(data.index, pd.DatetimeIndex):\n",
    "            time_diff = data.index.to_series().diff()\n",
    "            if symbol in config['crypto_symbols']:\n",
    "                gaps = time_diff[time_diff > pd.Timedelta(hours=2)]\n",
    "            else:\n",
    "                gaps = time_diff[time_diff > pd.Timedelta(days=4)]\n",
    "            if len(gaps) > 0:\n",
    "                issues.append(f\"{len(gaps)} time gaps detected\")\n",
    "        # Check for outliers\n",
    "        if 'close' in data.columns:\n",
    "            returns = data['close'].pct_change()\n",
    "            extreme_returns = returns[abs(returns) > 0.5]  # 50% moves\n",
    "            if len(extreme_returns) > 0:\n",
    "                issues.append(f\"{len(extreme_returns)} extreme price moves\")\n",
    "        # Check for zero/negative prices\n",
    "        price_cols = ['open', 'high', 'low', 'close']\n",
    "        for col in price_cols:\n",
    "            if col in data.columns:\n",
    "                invalid_prices = data[data[col] <= 0]\n",
    "                if len(invalid_prices) > 0:\n",
    "                    issues.append(f\"{len(invalid_prices)} invalid {col} prices\")\n",
    "        # Export the data to parquet for caching\n",
    "        file_path = export_path / f\"{symbol.lower()}_data.parquet\"\n",
    "        data.to_parquet(file_path)\n",
    "        # Record validation results\n",
    "        validation_results.append({\n",
    "            'Symbol': symbol,\n",
    "            'Records': len(data),\n",
    "            'Start': data.index.min(),\n",
    "            'End': data.index.max(),\n",
    "            'Issues': '; '.join(issues) if issues else 'None',\n",
    "            'Status': 'Warning' if issues else 'Clean',\n",
    "            'File': str(file_path)\n",
    "        })\n",
    "        \n",
    "        status = '[X]' if issues else '[O]'\n",
    "        print(f\"{status} {symbol}: Exported {len(data)} records to {file_path.name}\")\n",
    "        if issues:\n",
    "            for issue in issues:\n",
    "                print(f\"    - {issue}\")\n",
    "    \n",
    "    validation_df = pd.DataFrame(validation_results)\n",
    "    report_path = export_path / \"data_validation_report.csv\"\n",
    "    validation_df.to_csv(report_path, index=False)\n",
    "        \n",
    "    # Summary statistics\n",
    "    print(\"EXPORT SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    print(validation_df[['Symbol', 'Records', 'Status']].to_string(index=False))\n",
    "    \n",
    "    clean_count = len(validation_df[validation_df['Status'] == 'Clean'])\n",
    "    warning_count = len(validation_df[validation_df['Status'] == 'Warning'])\n",
    "    \n",
    "    print(f\"\\nClean datasets: {clean_count}\")\n",
    "    print(f\"Datasets with warnings: {warning_count}\")\n",
    "    \n",
    "    return validation_df\n",
    "\n",
    "# Validate and export all collected data\n",
    "if 'test_data' in locals() and test_data:\n",
    "    validation_report = validate_and_export_data(test_data)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"DATA COLLECTION PIPELINE COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Successfully collected and exported {len(test_data)} datasets\")\n",
    "    print(f\"Data saved to: ../data/processed/\")\n",
    "else:\n",
    "    print(\" No data available to export.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3041974",
   "metadata": {},
   "source": [
    "#### 8. Data Export and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "837db3b7",
   "metadata": {
    "title": "Create summary report"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Collection Summary Report\n",
      "================================================================================\n",
      "   Symbol  Records       Start         End  Days  Avg Close Volatility\n",
      "0  SOLUSD  1354459  2023-01-01  2025-08-01   943    $108.58       2.2%\n",
      "1  XRPUSD  1311440  2023-01-01  2025-08-01   943      $1.05       2.2%\n",
      "2  BTCUSD    22656  2023-01-01  2025-08-01   943  $59001.45       8.1%\n",
      "3  ETHUSD    22656  2023-01-01  2025-08-01   943   $2449.31      10.4%\n",
      "4  ADAUSD    22656  2023-01-01  2025-08-01   943      $0.51      15.9%\n",
      "5     SPY      942  2023-01-03  2025-08-01   941    $507.89      13.3%\n",
      "6     QQQ      942  2023-01-03  2025-08-01   941    $429.45      17.5%\n",
      "7     IWM      942  2023-01-03  2025-08-01   941    $200.35      18.3%\n",
      "8     DIA      942  2023-01-03  2025-08-01   941    $385.15      11.8%\n",
      "9     VTI      942  2023-01-03  2025-08-01   941    $252.17      13.6%\n",
      "\n",
      "Summary saved to ..\\data\\ml_comparison_cache\\data_summary.csv\n"
     ]
    }
   ],
   "source": [
    "def create_summary_report(data_dict):\n",
    "    print(\"Data Collection Summary Report\")\n",
    "    print(\"=\"*80)\n",
    "    if not data_dict:\n",
    "        print(\"No data to summarize\")\n",
    "        return\n",
    "    summary = [] \n",
    "    for symbol, data in data_dict.items():\n",
    "        if isinstance(data, pd.DataFrame) and not data.empty:\n",
    "            # Handle both DatetimeIndex and regular datetime columns\n",
    "            if isinstance(data.index, pd.DatetimeIndex):\n",
    "                start_date = data.index.min()\n",
    "                end_date = data.index.max()\n",
    "            elif 'timestamp' in data.columns:\n",
    "                start_date = pd.to_datetime(data['timestamp'].min())\n",
    "                end_date = pd.to_datetime(data['timestamp'].max())\n",
    "            else:\n",
    "                # Try to convert index to datetime\n",
    "                try:\n",
    "                    start_date = pd.to_datetime(data.index.min())\n",
    "                    end_date = pd.to_datetime(data.index.max())\n",
    "                except:\n",
    "                    start_date = None\n",
    "                    end_date = None\n",
    "            summary.append({\n",
    "                'Symbol': symbol, 'Records': len(data),\n",
    "                'Start': start_date.strftime('%Y-%m-%d') if start_date else 'N/A',\n",
    "                'End': end_date.strftime('%Y-%m-%d') if end_date else 'N/A',\n",
    "                'Days': (end_date - start_date).days if start_date and end_date else 'N/A',\n",
    "                'Avg Close': f\"${data['close'].mean():.2f}\" if 'close' in data.columns else 'N/A',\n",
    "                'Volatility': f\"{data['close'].pct_change().std() * np.sqrt(252) * 100:.1f}%\" if 'close' in data.columns else 'N/A'})\n",
    "    summary_df = pd.DataFrame(summary)\n",
    "    print(summary_df.to_string())\n",
    "    # Save to CSV\n",
    "    summary_path = Path(config['cache_dir']) / 'data_summary.csv'\n",
    "    summary_df.to_csv(summary_path, index=False)\n",
    "    print(f\"\\nSummary saved to {summary_path}\")\n",
    "    return summary_df\n",
    "if test_data:\n",
    "    summary = create_summary_report(test_data)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
