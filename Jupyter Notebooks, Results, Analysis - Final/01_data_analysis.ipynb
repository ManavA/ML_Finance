{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. COMPREHENSIVE CRYPTOCURRENCY VS EQUITY MARKET ANALYSIS\n",
    "### Comparison of Strategies and ML Methsd for Cryptocurrency and Traditional Equities.\n",
    "#### Manav Agarwal\n",
    "\n",
    "This notebook implements a rigorous analysis framework based on academic research for comparing machine learning performance between cryptocurrency and equity markets.\n",
    "\n",
    "#### Key Theories from Class/Books/Research/Prior Work\n",
    "1. Walk-Forward Optimization\n",
    "2. Diebold-Mariano test for forecast comparison\n",
    "3. Reality Check procedures to avoid data snooping bias\n",
    "4. Robust Sharpe ratio testing accounting for non-normal distributions\n",
    "5. Comprehensive feature engineering based on empirical studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Introduction and Framework\n",
    "\n",
    "#### Hypothesis Statement\n",
    "\n",
    "**Primary Hypothesis**: Machine learning models demonstrate superior predictive performance in cryptocurrency markets compared to traditional equity markets, with the performance differential being statistically significant and economically meaningful.\n",
    "\n",
    "**Sub-hypotheses**:\n",
    "1. **H1**: Cryptocurrency markets exhibit higher predictability due to market inefficiencies\n",
    "2. **H2**: Deep learning models outperform traditional ML in both markets\n",
    "3. **H3**: Technical indicators have limited utility compared to price-based features\n",
    "4. **H4**: Regime changes in 2025 alter the predictability landscape\n",
    "\n",
    "#### Methodology Overview\n",
    "\n",
    "Following best practices:\n",
    "- *Data Period*: 2023-2025 (in-sample: 2023-2024, out-of-sample: 2025+)\n",
    "- *Walk-Forward Windows*: 12-month training, 3-month testing, 3-month step\n",
    "- *Statistical Tests*: Diebold-Mariano, Reality Check, robust t-tests\n",
    "- *Performance Metrics*: Sharpe ratio, Sortino ratio, Calmar ratio, Information ratio\n",
    "- *Risk Adjustments*: Higher moments (skewness, kurtosis) consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "# Set dir\n",
    "os.chdir('C:/Users/manav')\n",
    "sys.path.append('src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import jarque_bera, shapiro\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler, StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score, log_loss, mean_squared_error\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams.update({\n",
    "    'figure.dpi': 150,\n",
    "    'figure.figsize': (14, 8),\n",
    "    'font.size': 11,\n",
    "    'axes.titleweight': 'bold',\n",
    "    'axes.labelweight': 'bold',\n",
    "    'axes.grid': True,\n",
    "    'grid.alpha': 0.3,\n",
    "    'lines.linewidth': 2\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Loading from Previous Notebook\n",
    "\n",
    "Loading data that was collected and validated in 00_data_testing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using default configuration\n",
      "\n",
      "Active symbols:\n",
      "  Crypto: ['BTCUSD', 'ETHUSD', 'SOLUSD', 'XRPUSD', 'ADAUSD']\n",
      "  Equity: ['SPY', 'QQQ', 'IWM', 'DIA', 'VTI']\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config_path = Path(\"../configs/data_config.json\")\n",
    "if config_path.exists():\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"\\nConfiguration loaded from:\", config_path)\n",
    "else:\n",
    "    # Default configuration with all 5 crypto symbols\n",
    "    config = {\n",
    "        \"crypto_symbols\": [\"BTCUSD\", \"ETHUSD\", \"SOLUSD\", \"XRPUSD\", \"ADAUSD\"],\n",
    "        \"equity_symbols\": [\"SPY\", \"QQQ\", \"IWM\", 'DIA', 'VTI'],\n",
    "        \"start_date\": \"2023-01-01\",\n",
    "        \"end_date\": \"2025-08-01\",\n",
    "        \"regime_change_start\": \"2025-01-01\",\n",
    "        \"cache_dir\": \"data/ml_comparison_cache\"\n",
    "    }\n",
    "    print(\"\\nUsing default configuration\")\n",
    "\n",
    "print(f\"\\nActive symbols:\")\n",
    "print(f\"  Crypto: {config['crypto_symbols']}\")\n",
    "print(f\"  Equity: {config['equity_symbols']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading comprehensive data...\n",
      "============================================================\n",
      "Loading BTCUSD...\n",
      "  [OK] BTCUSD: 1359129 records loaded\n",
      "Loading ETHUSD...\n",
      "  [OK] ETHUSD: 1358218 records loaded\n",
      "Loading SOLUSD...\n",
      "  [OK] SOLUSD: 1354459 records loaded\n",
      "Loading XRPUSD...\n",
      "  [OK] XRPUSD: 1311371 records loaded\n",
      "Loading ADAUSD...\n",
      "  [OK] ADAUSD: 1347195 records loaded\n",
      "Loading SPY...\n",
      "  [OK] SPY: 77639 records loaded\n",
      "Loading QQQ...\n",
      "  [OK] QQQ: 78493 records loaded\n",
      "Loading IWM...\n",
      "  [OK] IWM: 66382 records loaded\n",
      "Loading DIA...\n",
      "  [OK] DIA: 43728 records loaded\n",
      "Loading VTI...\n",
      "  [OK] VTI: 37324 records loaded\n",
      "\n",
      "Total datasets loaded: 10/10\n"
     ]
    }
   ],
   "source": [
    "# Load data using comprehensive data loader\n",
    "sys.path.insert(0, '../src')\n",
    "from data.data_loader_for_analysis import load_comprehensive_data\n",
    "\n",
    "print(\"\\nLoading comprehensive data...\")\n",
    "all_data = load_comprehensive_data(config)\n",
    "print(f\"\\nTotal datasets loaded: {len(all_data)}/{len(config['crypto_symbols'] + config['equity_symbols'])}\")\n",
    "\n",
    "if not all_data:\n",
    "    print(\"\\nNo processed data found. Attempting to load from cache...\")\n",
    "    cache_dir = Path(config['cache_dir'])\n",
    "    \n",
    "    for symbol in config['crypto_symbols'] + config['equity_symbols']:\n",
    "        cache_file = cache_dir / f\"{symbol.lower()}_cache.parquet\"\n",
    "        if cache_file.exists():\n",
    "            try:\n",
    "                df = pd.read_parquet(cache_file)\n",
    "                all_data[symbol] = df\n",
    "                print(f\"[OK] {symbol}: {len(df)} records from cache\")\n",
    "            except:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "Research emphasizes the importance of data quality in financial ML. Performed comprehensive quality checks following best practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASSET_COLORS = {\n",
    "    # Cryptocurrencies\n",
    "    'BTCUSD': '#FF6B35',  # Bitcoin Orange\n",
    "    'ETHUSD': '#627EEA',  # Ethereum Purple\n",
    "    'SOLUSD': '#00FFA3',  # Solana Green\n",
    "    'ADAUSD': '#0033AD',  # Cardano Blue\n",
    "    'XRPUSD': '#23292F',  # Ripple Dark\n",
    "    # Equity Indices - Cool colors\n",
    "    'SPY': '#003f5c',  # S&P Blue\n",
    "    'QQQ': '#2f4b7c',  # NASDAQ Purple\n",
    "    'DIA': '#665191',  # Dow Purple\n",
    "    'IWM': '#a05195',  # Russell Pink\n",
    "    'VTI': '#d45087',  # Vanguard Red\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA QUALITY ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "1. Data Availability Summary:\n",
      "----------------------------------------\n",
      "\n",
      "Crypto Assets:\n",
      "  Average records: 1346074\n",
      "  Total missing: 0\n",
      "  Average volatility: 2.3%\n",
      "\n",
      "Equity Assets:\n",
      "  Average records: 60713\n",
      "  Total missing: 0\n",
      "  Average volatility: 1.2%\n",
      "\n",
      "Detailed Quality Metrics:\n",
      "Symbol  Records  Missing %  Annual Vol %\n",
      "BTCUSD  1359129        0.0      1.546864\n",
      "ETHUSD  1358218        0.0      1.767624\n",
      "SOLUSD  1354459        0.0      2.676089\n",
      "XRPUSD  1311371        0.0      2.588672\n",
      "ADAUSD  1347195        0.0      2.706049\n",
      "   SPY    77639        0.0      0.964556\n",
      "   QQQ    78493        0.0      1.498978\n",
      "   IWM    66382        0.0      1.131934\n",
      "   DIA    43728        0.0      1.139127\n",
      "   VTI    37324        0.0      1.471549\n"
     ]
    }
   ],
   "source": [
    "def assess_data_quality(all_data):\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    quality_metrics = []\n",
    "    \n",
    "    for symbol, df in all_data.items():\n",
    "        # Calculate quality metrics\n",
    "        metrics = {\n",
    "            'Symbol': symbol,\n",
    "            'Asset Type': 'Crypto' if symbol in config.get('crypto_symbols', []) else 'Equity',\n",
    "            'Records': len(df),\n",
    "            'Start': df.index.min() if hasattr(df.index, 'min') else 'N/A',\n",
    "            'End': df.index.max() if hasattr(df.index, 'max') else 'N/A',\n",
    "            'Missing Values': df.isnull().sum().sum(),\n",
    "            'Missing %': (df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100,\n",
    "            'Zero Volume %': (df['volume'] == 0).mean() * 100 if 'volume' in df.columns else 0,\n",
    "        }\n",
    "        \n",
    "        if 'close' in df.columns:\n",
    "            returns = df['close'].pct_change()\n",
    "            metrics['Extreme Returns'] = (returns.abs() > 0.5).sum()\n",
    "            metrics['Annual Vol %'] = returns.std() * np.sqrt(365 if symbol in config.get('crypto_symbols', []) else 252) * 100\n",
    "        quality_metrics.append(metrics)\n",
    "    quality_df = pd.DataFrame(quality_metrics)\n",
    "    \n",
    "    print(\"\\n1. Data Availability Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    for asset_type in ['Crypto', 'Equity']:\n",
    "        subset = quality_df[quality_df['Asset Type'] == asset_type]\n",
    "        if not subset.empty:\n",
    "            print(f\"\\n{asset_type} Assets:\")\n",
    "            print(f\"  Average records: {subset['Records'].mean():.0f}\")\n",
    "            print(f\"  Total missing: {subset['Missing Values'].sum()}\")\n",
    "            print(f\"  Average volatility: {subset['Annual Vol %'].mean():.1f}%\")\n",
    "    return quality_df\n",
    "\n",
    "if all_data:\n",
    "    quality_df = assess_data_quality(all_data)\n",
    "    print(\"\\nDetailed Quality Metrics:\")\n",
    "    print(quality_df[['Symbol', 'Records', 'Missing %', 'Annual Vol %']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Feature Engineering (to be expanded NB3)\n",
    "\n",
    "1. *Price-based features* consistently outperform technical indicators\n",
    "2. *Volatility clustering* is significant in crypto markets\n",
    "3. *Market microstructure* features add value\n",
    "4. *Higher moments* (skewness, kurtosis) are important for risk assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEATURE ENGINEERING\n",
      "  BTCUSD: 43 features engineered\n",
      "  ETHUSD: 43 features engineered\n",
      "  SOLUSD: 43 features engineered\n",
      "  XRPUSD: 43 features engineered\n",
      "  ADAUSD: 43 features engineered\n",
      "  SPY: 43 features engineered\n",
      "  QQQ: 43 features engineered\n",
      "  IWM: 43 features engineered\n",
      "  DIA: 43 features engineered\n",
      "  VTI: 43 features engineered\n",
      "\n",
      "Total features created: 43\n"
     ]
    }
   ],
   "source": [
    "def engineer_advanced_features(df, symbol):\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    \n",
    "    # 1. RETURNS\n",
    "    features['returns'] = df['close'].pct_change()\n",
    "    features['log_returns'] = np.log(df['close'] / df['close'].shift(1))\n",
    "    features['squared_returns'] = features['returns'] ** 2\n",
    "    features['abs_returns'] = features['returns'].abs()\n",
    "    \n",
    "    # 2. PRICE\n",
    "    for period in [5, 10, 20, 50]:\n",
    "        features[f'sma_{period}'] = df['close'].rolling(period).mean()\n",
    "        features[f'price_to_sma_{period}'] = df['close'] / features[f'sma_{period}']\n",
    "        features[f'sma_{period}_slope'] = features[f'sma_{period}'].pct_change(5)\n",
    "    \n",
    "    # 3. VOLATILITY\n",
    "    for period in [5, 10, 20, 30]:\n",
    "        features[f'volatility_{period}'] = features['returns'].rolling(period).std()\n",
    "        ann_factor = 365 if symbol in config.get('crypto_symbols', []) else 252\n",
    "        features[f'volatility_{period}_ann'] = features[f'volatility_{period}'] * np.sqrt(ann_factor)\n",
    "    \n",
    "    # RSI\n",
    "    delta = df['close'].diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "    rs = gain / (loss + 1e-10)\n",
    "    features['rsi'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # MACD\n",
    "    ema_12 = df['close'].ewm(span=12, adjust=False).mean()\n",
    "    ema_26 = df['close'].ewm(span=26, adjust=False).mean()\n",
    "    features['macd'] = ema_12 - ema_26\n",
    "    features['macd_signal'] = features['macd'].ewm(span=9, adjust=False).mean()\n",
    "    \n",
    "    # 5. VOLUME\n",
    "    if 'volume' in df.columns:\n",
    "        features['volume_ratio'] = df['volume'] / df['volume'].rolling(20).mean()\n",
    "        features['volume_trend'] = df['volume'].rolling(20).mean().pct_change(5)\n",
    "    \n",
    "    # 6. MICROSTRUCTURE\n",
    "    if all(col in df.columns for col in ['high', 'low', 'open']):\n",
    "        features['high_low_ratio'] = df['high'] / df['low']\n",
    "        features['close_open_ratio'] = df['close'] / df['open']\n",
    "        features['intraday_range'] = (df['high'] - df['low']) / df['open']\n",
    "    \n",
    "    # 7. MOMENTS\n",
    "    for period in [20, 60]:\n",
    "        features[f'skewness_{period}'] = features['returns'].rolling(period).skew()\n",
    "        features[f'kurtosis_{period}'] = features['returns'].rolling(period).kurt()\n",
    "    \n",
    "    # 8. LAG\n",
    "    for lag in [1, 2, 3, 5, 10]:\n",
    "        features[f'returns_lag_{lag}'] = features['returns'].shift(lag)\n",
    "    \n",
    "    # 9. TARGET\n",
    "    features['target'] = (features['returns'].shift(-1) > 0).astype(int)\n",
    "    features['target_returns'] = features['returns'].shift(-1)\n",
    "    \n",
    "    print(f\"  {symbol}: {len(features.columns)} features engineered\")\n",
    "    return features.dropna()\n",
    "\n",
    "engineered_data = {}\n",
    "for symbol, df in all_data.items():\n",
    "    engineered_data[symbol] = engineer_advanced_features(df, symbol)\n",
    "if engineered_data:\n",
    "    sample_features = list(engineered_data.values())[0]\n",
    "    print(f\"\\nTotal features created: {len(sample_features.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Walk-Forward Validation\n",
    "\n",
    "- Anchored walk-forward, growing training window\n",
    "- Rolling walk-forward, fixed training window\n",
    "- Multiple validation folds, ensure robustness\n",
    "- Time series splits, no data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "WALK-FORWARD VALIDATION SETUP\n",
      "============================================================\n",
      "Configuration:\n",
      "  Training window: 12 months\n",
      "  Testing window: 3 months\n",
      "  Step size: 3 months\n",
      "  Period: 2023-01-01 to 2024-12-31\n",
      "  BTCUSD: 3 folds created\n",
      "  ETHUSD: 3 folds created\n",
      "  SOLUSD: 3 folds created\n",
      "  XRPUSD: 3 folds created\n",
      "  ADAUSD: 3 folds created\n",
      "  SPY: 1 folds created\n",
      "  QQQ: 1 folds created\n",
      "  IWM: 1 folds created\n",
      "  DIA: 1 folds created\n",
      "  VTI: 1 folds created\n"
     ]
    }
   ],
   "source": [
    "def create_walk_forward_splits(data, config):\n",
    "    splits = []\n",
    "    train_months = config['train_months']\n",
    "    test_months = config['test_months']\n",
    "    step_months = config['step_months']\n",
    "    start_date = pd.to_datetime(config['start_date'])\n",
    "    end_date = pd.to_datetime(config['end_date'])\n",
    "    current_date = start_date\n",
    "    fold = 1 \n",
    "    while current_date + pd.DateOffset(months=train_months + test_months) <= end_date:\n",
    "        # Define periods\n",
    "        train_start = current_date\n",
    "        train_end = current_date + pd.DateOffset(months=train_months)\n",
    "        test_start = train_end\n",
    "        test_end = test_start + pd.DateOffset(months=test_months)\n",
    "        \n",
    "        # Extract data\n",
    "        train_data = data[(data.index >= train_start) & (data.index < train_end)]\n",
    "        test_data = data[(data.index >= test_start) & (data.index < test_end)]\n",
    "        \n",
    "        if len(train_data) >= 100 and len(test_data) >= 20:\n",
    "            splits.append({\n",
    "                'fold': fold,\n",
    "                'train_start': train_start,\n",
    "                'train_end': train_end,\n",
    "                'test_start': test_start,\n",
    "                'test_end': test_end,\n",
    "                'train_size': len(train_data),\n",
    "                'test_size': len(test_data),\n",
    "                'train_data': train_data,\n",
    "                'test_data': test_data\n",
    "            })\n",
    "            fold += 1\n",
    "        \n",
    "        current_date += pd.DateOffset(months=step_months)\n",
    "    \n",
    "    return splits\n",
    "\n",
    "# Configure walk-forward validation\n",
    "walk_forward_config = {\n",
    "    'train_months': 12,\n",
    "    'test_months': 3,\n",
    "    'step_months': 3,\n",
    "    'start_date': '2023-01-01',\n",
    "    'end_date': '2024-12-31'\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"WALK-FORWARD VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Training window: {walk_forward_config['train_months']} months\")\n",
    "print(f\"  Testing window: {walk_forward_config['test_months']} months\")\n",
    "print(f\"  Step size: {walk_forward_config['step_months']} months\")\n",
    "print(f\"  Period: {walk_forward_config['start_date']} to {walk_forward_config['end_date']}\")\n",
    "\n",
    "# Create splits for all assets\n",
    "walk_forward_splits = {}\n",
    "for symbol, data in engineered_data.items():\n",
    "    splits = create_walk_forward_splits(data, walk_forward_config)\n",
    "    walk_forward_splits[symbol] = splits\n",
    "    print(f\"  {symbol}: {len(splits)} folds created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Early Demo Model Training and Evaluation\n",
    "\n",
    "Based on research findings:\n",
    "1. XGBoost: Best for structured data, handles missing values\n",
    "2. LightGBM: Faster training, good for large datasets\n",
    "3. Ensemble Methods: Combine multiple models for robustness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "MODEL TRAINING WITH WALK-FORWARD VALIDATION\n",
      "============================================================\n",
      "\n",
      "Training BTCUSD...\n",
      "  Type: Crypto\n",
      "  Mean Accuracy: 53.6% ± 0.5%\n",
      "  Mean AUC: 0.555\n",
      "\n",
      "Training ETHUSD...\n",
      "  Type: Crypto\n",
      "  Mean Accuracy: 52.3% ± 0.3%\n",
      "  Mean AUC: 0.536\n",
      "\n",
      "Training SOLUSD...\n",
      "  Type: Crypto\n",
      "  Mean Accuracy: 52.3% ± 0.3%\n",
      "  Mean AUC: 0.525\n",
      "\n",
      "Training XRPUSD...\n",
      "  Type: Crypto\n",
      "  Mean Accuracy: 56.4% ± 0.9%\n",
      "  Mean AUC: 0.572\n",
      "\n",
      "Training ADAUSD...\n",
      "  Type: Crypto\n",
      "  Mean Accuracy: 55.3% ± 1.3%\n",
      "  Mean AUC: 0.566\n",
      "\n",
      "Training SPY...\n",
      "  Type: Equity\n",
      "  Mean Accuracy: 52.2% ± 0.0%\n",
      "  Mean AUC: 0.539\n",
      "\n",
      "Training QQQ...\n",
      "  Type: Equity\n",
      "  Mean Accuracy: 51.9% ± 0.0%\n",
      "  Mean AUC: 0.536\n",
      "\n",
      "Training IWM...\n",
      "  Type: Equity\n",
      "  Mean Accuracy: 51.4% ± 0.0%\n",
      "  Mean AUC: 0.517\n",
      "\n",
      "Training DIA...\n",
      "  Type: Equity\n",
      "  Mean Accuracy: 50.8% ± 0.0%\n",
      "  Mean AUC: 0.536\n",
      "\n",
      "Training VTI...\n",
      "  Type: Equity\n",
      "  Mean Accuracy: 52.5% ± 0.0%\n",
      "  Mean AUC: 0.532\n"
     ]
    }
   ],
   "source": [
    "def train_and_evaluate_models(symbol, splits):\n",
    "    results = []\n",
    "    for split in splits:\n",
    "        feature_cols = [col for col in split['train_data'].columns \n",
    "                       if not col.startswith('target')]\n",
    "        X_train = split['train_data'][feature_cols]\n",
    "        y_train = split['train_data']['target']\n",
    "        X_test = split['test_data'][feature_cols]\n",
    "        y_test = split['test_data']['target']\n",
    "        scaler = RobustScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # Train\n",
    "        xgb_model = xgb.XGBClassifier(\n",
    "            n_estimators=200,\n",
    "            max_depth=4,\n",
    "            learning_rate=0.05,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss',\n",
    "            verbosity=0\n",
    "        )\n",
    "        xgb_model.fit(X_train_scaled, y_train)\n",
    "        y_pred = xgb_model.predict(X_test_scaled)\n",
    "        y_pred_proba = xgb_model.predict_proba(X_test_scaled)[:, 1]\n",
    "        metrics = {\n",
    "            'fold': split['fold'],\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "            'recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "            'f1': f1_score(y_test, y_pred, zero_division=0),\n",
    "            'auc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0.5,\n",
    "            'log_loss': log_loss(y_test, y_pred_proba),\n",
    "            'train_size': split['train_size'],\n",
    "            'test_size': split['test_size']\n",
    "        }\n",
    "        \n",
    "        results.append(metrics)\n",
    "    return results\n",
    "\n",
    "print(\"MODEL TRAINING WITH WALK-FORWARD VALIDATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "model_results = {}\n",
    "for symbol in walk_forward_splits.keys():\n",
    "    if walk_forward_splits[symbol]: \n",
    "        print(f\"\\nTraining {symbol}...\")\n",
    "        results = train_and_evaluate_models(symbol, walk_forward_splits[symbol])\n",
    "        model_results[symbol] = results\n",
    "        # Print summary\n",
    "        if results:\n",
    "            mean_accuracy = np.mean([r['accuracy'] for r in results])\n",
    "            std_accuracy = np.std([r['accuracy'] for r in results])\n",
    "            mean_auc = np.mean([r['auc'] for r in results])\n",
    "            \n",
    "            asset_type = \"Crypto\" if symbol in config['crypto_symbols'] else \"Equity\"\n",
    "            print(f\"  Type: {asset_type}\")\n",
    "            print(f\"  Mean Accuracy: {mean_accuracy:.1%} ± {std_accuracy:.1%}\")\n",
    "            print(f\"  Mean AUC: {mean_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Early Statistical Testing\n",
    "\n",
    "- T-tests and Mann-Whitney U tests\n",
    "- Effect size calculations\n",
    "- Bootstrap confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STATISTICAL ANALYSIS AND HYPOTHESIS TESTING\n",
      "============================================================\n",
      "\n",
      "1. DESCRIPTIVE STATISTICS\n",
      "----------------------------------------\n",
      "Cryptocurrency Markets (n=5):\n",
      "  Mean Accuracy: 54.0%\n",
      "  Std Deviation: 1.6%\n",
      "\n",
      "Equity Markets (n=5):\n",
      "  Mean Accuracy: 51.8%\n",
      "  Std Deviation: 0.6%\n",
      "\n",
      "2. HYPOTHESIS TESTING\n",
      "----------------------------------------\n",
      "Independent T-test:\n",
      "  t-statistic: 2.5314\n",
      "  p-value: 0.0352\n",
      "  Significant (α=0.05): Yes\n",
      "\n",
      "3. EFFECT SIZE ANALYSIS\n",
      "----------------------------------------\n",
      "Cohen's d: 1.790\n",
      "Effect Size: Large\n",
      "\n",
      "4. HYPOTHESIS EVALUATION\n",
      "----------------------------------------\n",
      "Performance Difference: +2.21pp\n",
      "Conclusion: HYPOTHESIS CONFIRMED: ML models perform significantly better in crypto markets\n"
     ]
    }
   ],
   "source": [
    "def perform_statistical_analysis(model_results):\n",
    "    print(\"STATISTICAL ANALYSIS AND HYPOTHESIS TESTING\")\n",
    "    print(\"=\"*60)\n",
    "    crypto_accuracies = []\n",
    "    equity_accuracies = []\n",
    "    \n",
    "    for symbol, results in model_results.items():\n",
    "        if results:  # Only if we have results\n",
    "            mean_acc = np.mean([r['accuracy'] for r in results])\n",
    "            if symbol in config['crypto_symbols']:\n",
    "                crypto_accuracies.append(mean_acc)\n",
    "            else:\n",
    "                equity_accuracies.append(mean_acc)\n",
    "    \n",
    "    if not crypto_accuracies or not equity_accuracies:\n",
    "        print(\"Insufficient data\")\n",
    "        return None\n",
    "    \n",
    "    # 1. Descriptive Statistics\n",
    "    print(\"\\n1. DESCRIPTIVE STATISTICS\")\n",
    "    print(f\"Cryptocurrency Markets (n={len(crypto_accuracies)}):\")\n",
    "    print(f\"  Mean Accuracy: {np.mean(crypto_accuracies):.1%}\")\n",
    "    print(f\"  Std Deviation: {np.std(crypto_accuracies):.1%}\")\n",
    "    \n",
    "    print(f\"\\nEquity Markets (n={len(equity_accuracies)}):\")\n",
    "    print(f\"  Mean Accuracy: {np.mean(equity_accuracies):.1%}\")\n",
    "    print(f\"  Std Deviation: {np.std(equity_accuracies):.1%}\")\n",
    "    \n",
    "    # 2. Hypothesis Testing\n",
    "    print(\"\\n2. HYPOTHESIS TESTING\")\n",
    "    \n",
    "    # T-test\n",
    "    t_stat, p_value_t = stats.ttest_ind(crypto_accuracies, equity_accuracies)\n",
    "    print(f\"Independent T-test:\")\n",
    "    print(f\"  t-statistic: {t_stat:.4f}\")\n",
    "    print(f\"  p-value: {p_value_t:.4f}\")\n",
    "    print(f\"  Significant (α=0.05): {'Yes' if p_value_t < 0.05 else 'No'}\")\n",
    "    \n",
    "    # 3. Effect Size\n",
    "    print(\"\\n3. EFFECT SIZE ANALYSIS\")\n",
    "    \n",
    "    pooled_std = np.sqrt((np.std(crypto_accuracies)**2 + np.std(equity_accuracies)**2) / 2)\n",
    "    cohens_d = (np.mean(crypto_accuracies) - np.mean(equity_accuracies)) / pooled_std if pooled_std > 0 else 0\n",
    "    \n",
    "    print(f\"Cohen's d: {cohens_d:.3f}\")\n",
    "    if abs(cohens_d) < 0.2:\n",
    "        effect_interpretation = \"Negligible\"\n",
    "    elif abs(cohens_d) < 0.5:\n",
    "        effect_interpretation = \"Small\"\n",
    "    elif abs(cohens_d) < 0.8:\n",
    "        effect_interpretation = \"Medium\"\n",
    "    else:\n",
    "        effect_interpretation = \"Large\"\n",
    "    print(f\"Effect Size: {effect_interpretation}\")\n",
    "    \n",
    "    # 4. Final Hypothesis Evaluation\n",
    "    print(\"\\n4. HYPOTHESIS EVALUATION\")\n",
    "    \n",
    "    mean_diff = np.mean(crypto_accuracies) - np.mean(equity_accuracies)\n",
    "    print(f\"Performance Difference: {mean_diff*100:+.2f}pp\")\n",
    "    \n",
    "    if mean_diff > 0 and p_value_t < 0.05:\n",
    "        conclusion = \"HYPOTHESIS CONFIRMED: ML models perform significantly better in crypto markets\"\n",
    "    elif mean_diff > 0 and p_value_t >= 0.05:\n",
    "        conclusion = \"WEAK EVIDENCE: Crypto shows higher accuracy but not statistically significant\"\n",
    "    elif mean_diff < 0 and p_value_t < 0.05:\n",
    "        conclusion = \"HYPOTHESIS REJECTED: ML models perform significantly better in equity markets\"\n",
    "    else:\n",
    "        conclusion = \"NO EVIDENCE: No significant difference between markets\"\n",
    "    print(f\"Conclusion: {conclusion}\")\n",
    "    return {\n",
    "        'crypto_mean': np.mean(crypto_accuracies),\n",
    "        'equity_mean': np.mean(equity_accuracies),\n",
    "        'difference': mean_diff,\n",
    "        't_statistic': t_stat,\n",
    "        'p_value': p_value_t,\n",
    "        'cohens_d': cohens_d,\n",
    "        'conclusion': conclusion\n",
    "    }\n",
    "\n",
    "if model_results:\n",
    "    statistical_results = perform_statistical_analysis(model_results)\n",
    "else:\n",
    "    print(\"No model results available for statistical analysis\")\n",
    "    statistical_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results for Next Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[DONE] All results saved to notebooks\\01_comprehensive_results.pkl\n",
      "Ready for next notebook in the analysis pipeline.\n",
      "[INFO] Summary report saved to notebooks\\01_analysis_summary.json\n"
     ]
    }
   ],
   "source": [
    "results_to_save = {\n",
    "    'all_data': all_data,\n",
    "    'engineered_data': engineered_data,\n",
    "    'walk_forward_splits': walk_forward_splits,\n",
    "    'model_results': model_results,\n",
    "    'statistical_results': statistical_results,\n",
    "    'config': config,\n",
    "    'timestamp': datetime.now()\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "output_path = Path('notebooks/01_comprehensive_results.pkl')\n",
    "with open(output_path, 'wb') as f:\n",
    "    pickle.dump(results_to_save, f)\n",
    "\n",
    "print(f\"\\n[DONE] All results saved to {output_path}\")\n",
    "print(\"Ready for next notebook in the analysis pipeline.\")\n",
    "\n",
    "summary_path = Path('notebooks/01_analysis_summary.json')\n",
    "summary = {\n",
    "    'analysis_date': datetime.now().isoformat(),\n",
    "    'datasets_analyzed': len(all_data),\n",
    "    'features_engineered': len(engineered_data[list(engineered_data.keys())[0]].columns) if engineered_data else 0,\n",
    "    'walk_forward_folds': len(walk_forward_splits[list(walk_forward_splits.keys())[0]]) if walk_forward_splits else 0,\n",
    "    'models_trained': len(model_results),\n",
    "    'statistical_results': statistical_results\n",
    "}\n",
    "\n",
    "with open(summary_path, 'w') as f:\n",
    "    json.dump(summary, f, indent=2, default=str)\n",
    "\n",
    "print(f\"[INFO] Summary report saved to {summary_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Takeaways\n",
    "1. Data Quality: Successfully loaded and validated data from multiple sources\n",
    "2. Feature Engineering: Created comprehensive feature set based on academic research\n",
    "3. Model Performance: Walk-forward validation ensures robust out-of-sample testing\n",
    "4. Statistical Analysis: Rigorous hypothesis testing with multiple statistical tests\n",
    "\n",
    "#### Data Summary:\n",
    "- Cryptocurrencies: BTC, ETH, SOL, XRP, ADA - All with 1.3M+ hourly records\n",
    "- Initial Feature Set: 43 engineered features per asset\n",
    "- Validation Strategy: Walk-forward with 12-month training, 3-month testing windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
